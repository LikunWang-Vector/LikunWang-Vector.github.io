#!/usr/bin/env python3
"""
CSDN to Hexo Blog Migration Script
çˆ¬å–CSDNæ–‡ç« å®Œæ•´å†…å®¹å¹¶è½¬æ¢ä¸ºHexoåšå®¢æ ¼å¼
"""

import re
import os
import json
import time
import requests
from bs4 import BeautifulSoup
import html2text

# é…ç½®
OUTPUT_DIR = "source/_posts"
ARTICLES_JSON = "articles.json"
DELAY = 2  # è¯·æ±‚é—´éš”(ç§’)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Referer': 'https://blog.csdn.net/',
}

# HTML to Markdown è½¬æ¢å™¨
h2t = html2text.HTML2Text()
h2t.ignore_links = False
h2t.ignore_images = False
h2t.body_width = 0  # ä¸æ¢è¡Œ


def extract_urls_from_data(html_file):
    """ä»dataæ–‡ä»¶æå–æ‰€æœ‰æ–‡ç« URLå’ŒåŸºæœ¬ä¿¡æ¯"""
    with open(html_file, 'r', encoding='utf-8') as f:
        html = f.read()
    
    soup = BeautifulSoup(html, 'html.parser')
    articles = []
    
    for block in soup.find_all('article', class_='blog-list-box'):
        article = {}
        
        # URL
        link = block.find('a', href=True)
        if link and '/article/details/' in link['href']:
            article['url'] = link['href']
            match = re.search(r'/article/details/(\d+)', article['url'])
            if match:
                article['id'] = match.group(1)
        
        # æ ‡é¢˜
        h4 = block.find('h4')
        if h4:
            article['title'] = h4.get_text(strip=True)
        
        # æ—¥æœŸ
        date_div = block.find('div', class_='view-time-box')
        if date_div:
            date_match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})', date_div.get_text())
            if date_match:
                article['date'] = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
        
        # é˜…è¯»æ•°
        views_div = block.find('div', class_='view-num-box')
        if views_div:
            m = re.search(r'(\d+)', views_div.get_text())
            if m:
                article['views'] = int(m.group(1))
        
        # ç‚¹èµæ•°
        likes_div = block.find('div', class_='give-like-box')
        if likes_div:
            m = re.search(r'(\d+)', likes_div.get_text())
            if m:
                article['likes'] = int(m.group(1))
        
        # è¯„è®ºå’Œæ”¶è—
        comment_divs = block.find_all('div', class_='comment-box')
        if len(comment_divs) >= 1:
            m = re.search(r'(\d+)', comment_divs[0].get_text())
            if m:
                article['comments'] = int(m.group(1))
        if len(comment_divs) >= 2:
            m = re.search(r'(\d+)', comment_divs[1].get_text())
            if m:
                article['favorites'] = int(m.group(1))
        
        if article.get('url') and article.get('title'):
            articles.append(article)
    
    return articles


def fetch_article_detail(url):
    """çˆ¬å–å•ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹"""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        result = {
            'content': '',
            'tags': [],
            'category': 'General',
            'cover': None
        }
        
        # è·å–æ–‡ç« å†…å®¹
        content_div = soup.find('div', id='content_views')
        if not content_div:
            content_div = soup.find('article', class_='baidu_pl')
        if not content_div:
            content_div = soup.find('div', class_='article_content')
        
        if content_div:
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for elem in content_div.find_all(['script', 'style', 'iframe']):
                elem.decompose()
            
            # å¤„ç†ä»£ç å—ï¼Œä¿ç•™è¯­è¨€æ ‡è¯†
            for pre in content_div.find_all('pre'):
                code = pre.find('code')
                if code:
                    lang = ''
                    if code.get('class'):
                        for cls in code['class']:
                            if 'language-' in cls or 'lang-' in cls:
                                lang = cls.replace('language-', '').replace('lang-', '')
                                break
                            if cls in ['python', 'java', 'javascript', 'js', 'cpp', 'c', 'html', 'css', 'sql', 'bash', 'shell', 'json', 'xml', 'yaml']:
                                lang = cls
                                break
                    code_text = code.get_text()
                    # ç”¨ç‰¹æ®Šæ ‡è®°æ›¿æ¢ï¼Œåé¢è½¬æ¢
                    pre.replace_with(f'\n```{lang}\n{code_text}\n```\n')
            
            # è½¬æ¢ä¸ºMarkdown
            html_content = str(content_div)
            result['content'] = h2t.handle(html_content)
            
            # æ¸…ç†å¤šä½™ç©ºè¡Œ
            result['content'] = re.sub(r'\n{3,}', '\n\n', result['content'])
        
        # è·å–æ ‡ç­¾ - ä»blog-tags-boxä¸­æŸ¥æ‰¾tag-link-new
        tag_box = soup.find('div', class_='blog-tags-box')
        if tag_box:
            for tag_link in tag_box.find_all('a', class_='tag-link-new'):
                tag = tag_link.get_text(strip=True).lstrip('#')
                if tag and tag not in result['tags']:
                    result['tags'].append(tag)
        
        # å¤‡ç”¨æ–¹æ³•ï¼šä»tags-boxæŸ¥æ‰¾
        if not result['tags']:
            tag_box = soup.find('div', class_='tags-box')
            if tag_box:
                for tag_link in tag_box.find_all('a'):
                    tag = tag_link.get_text(strip=True).lstrip('#')
                    if tag and tag not in result['tags'] and len(tag) < 30:
                        result['tags'].append(tag)
        
        # è·å–åˆ†ç±»/ä¸“æ 
        column_link = soup.find('a', class_='column_name')
        if column_link:
            cat = column_link.get_text(strip=True)
            if cat and cat not in ['åŸåˆ›', 'è½¬è½½']:
                result['category'] = cat
        
        # è·å–å°é¢å›¾
        cover_img = soup.find('img', class_='article-cover')
        if not cover_img:
            # å°è¯•è·å–æ–‡ç« ä¸­ç¬¬ä¸€å¼ å›¾ç‰‡ä½œä¸ºå°é¢
            first_img = content_div.find('img') if content_div else None
            if first_img and first_img.get('src'):
                result['cover'] = first_img['src']
        else:
            result['cover'] = cover_img.get('src')
        
        return result
        
    except Exception as e:
        print(f"    é”™è¯¯: {e}")
        return {'content': '', 'tags': [], 'category': 'General', 'cover': None}


def sanitize_filename(title):
    """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
    filename = re.sub(r'[<>:"/\\|?*]', '', title)
    filename = re.sub(r'\s+', '-', filename)
    filename = filename.strip('-')
    if len(filename) > 80:
        filename = filename[:80]
    return filename


def create_hexo_post(article, detail):
    """ç”ŸæˆHexoåšå®¢æ–‡ç« """
    title = article.get('title', 'Untitled').replace('"', '\\"')
    date = article.get('date', '2023-01-01')
    category = detail.get('category', 'General')
    tags = detail.get('tags', [])
    content = detail.get('content', '')
    
    # Front matter
    front_matter = f'''---
title: "{title}"
date: {date}
updated: {date}
categories:
  - {category}
'''
    
    if tags:
        front_matter += 'tags:\n'
        for tag in tags:
            front_matter += f'  - {tag}\n'
    
    # CSDNç»Ÿè®¡æ•°æ®
    front_matter += f'''csdn_views: {article.get('views', 0)}
csdn_likes: {article.get('likes', 0)}
csdn_comments: {article.get('comments', 0)}
csdn_favorites: {article.get('favorites', 0)}
csdn_url: {article.get('url', '')}
'''
    
    if detail.get('cover'):
        front_matter += f"cover: {detail['cover']}\n"
    
    front_matter += '---\n\n'
    
    # åŸæ–‡ä¿¡æ¯æç¤º
    notice = f'''> æœ¬æ–‡è¿ç§»è‡ªCSDNåšå®¢
> åŸæ–‡é“¾æ¥ï¼š[{article.get('title', '')}]({article.get('url', '')})
> ğŸ“Š {article.get('views', 0)} é˜…è¯» | ğŸ‘ {article.get('likes', 0)} ç‚¹èµ | ğŸ’¬ {article.get('comments', 0)} è¯„è®º | â­ {article.get('favorites', 0)} æ”¶è—

'''
    
    return front_matter + notice + content


def main():
    print("=" * 60)
    print("CSDN åšå®¢è¿ç§»å·¥å…·")
    print("=" * 60)
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # æ­¥éª¤1: ä»dataæ–‡ä»¶æå–URL
    print("\n[æ­¥éª¤1] ä» data æ–‡ä»¶æå–æ–‡ç« URL...")
    articles = extract_urls_from_data('data')
    print(f"å…±å‘ç° {len(articles)} ç¯‡æ–‡ç« ")
    
    # ä¿å­˜å…ƒæ•°æ®
    with open(ARTICLES_JSON, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    
    # æ­¥éª¤2: çˆ¬å–æ¯ç¯‡æ–‡ç« 
    print(f"\n[æ­¥éª¤2] å¼€å§‹çˆ¬å–æ–‡ç« å†…å®¹ (é—´éš”{DELAY}ç§’)...")
    
    success = 0
    failed = 0
    
    for i, article in enumerate(articles, 1):
        title = article.get('title', 'Unknown')[:40]
        print(f"[{i}/{len(articles)}] {title}...", end=' ')
        
        try:
            # çˆ¬å–æ–‡ç« è¯¦æƒ…
            detail = fetch_article_detail(article['url'])
            
            if detail['content']:
                # ç”Ÿæˆå¹¶ä¿å­˜æ–‡ç« 
                filename = sanitize_filename(article['title']) + '.md'
                filepath = os.path.join(OUTPUT_DIR, filename)
                
                post_content = create_hexo_post(article, detail)
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(post_content)
                
                print(f"âœ“ ({len(detail['content'])}å­—)")
                success += 1
            else:
                print("âœ— å†…å®¹ä¸ºç©º")
                failed += 1
            
            # å»¶è¿Ÿï¼Œé¿å…è¢«å°
            time.sleep(DELAY)
            
        except Exception as e:
            print(f"âœ— {e}")
            failed += 1
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("è¿ç§»å®Œæˆ!")
    print("=" * 60)
    print(f"æˆåŠŸ: {success} ç¯‡")
    print(f"å¤±è´¥: {failed} ç¯‡")
    print(f"\næ–‡ç« ä¿å­˜åœ¨: {OUTPUT_DIR}/")
    print("\nä¸‹ä¸€æ­¥:")
    print("  hexo generate")
    print("  hexo server")


if __name__ == '__main__':
    main()
