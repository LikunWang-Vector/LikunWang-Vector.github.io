---
title: "21. å®æˆ˜ï¼šå¤šçº¿ç¨‹+xpathæŠ“å–å¤§é‡èœä»·ä¿¡æ¯ï¼ˆå››ç§æ–¹æ³•ï¼‰"
date: 2023-01-08
updated: 2023-01-08
categories:
  - Pythonçˆ¬è™«å…¥é—¨ã€è¿›é˜¶ä¸å®æˆ˜
tags:
  - çˆ¬è™«
  - python
  - æ•°æ®åˆ†æ
csdn_views: 452
csdn_likes: 4
csdn_comments: 2
csdn_favorites: 6
csdn_url: https://blog.csdn.net/m0_59180666/article/details/128600206
cover: /images/posts/21.-å®æˆ˜ï¼šå¤šçº¿ç¨‹+xpathæŠ“å–å¤§é‡èœä»·ä¿¡æ¯ï¼ˆå››ç§æ–¹æ³•ï¼‰/d0d623aba268.png
lang_pair:
  en: "21. Practical: Multithreading + XPath for Bulk Vegetable Price Scraping (Four Methods)"
---

> æœ¬æ–‡è¿ç§»è‡ªCSDNåšå®¢
> åŸæ–‡é“¾æ¥ï¼š[21. å®æˆ˜ï¼šå¤šçº¿ç¨‹+xpathæŠ“å–å¤§é‡èœä»·ä¿¡æ¯ï¼ˆå››ç§æ–¹æ³•ï¼‰](https://blog.csdn.net/m0_59180666/article/details/128600206)
> ğŸ“Š 452 é˜…è¯» | ğŸ‘ 4 ç‚¹èµ | ğŸ’¬ 2 è¯„è®º | â­ 6 æ”¶è—

**ç›®å½•**

å‰è¨€

ç›®çš„

æ€è·¯

ä»£ç å®ç°ï¼ˆå¤šçº¿ç¨‹+xpathï¼‰

1\. æŠ“å–å•ä¸ªé¡µé¢

2\. åˆ›å»ºçº¿ç¨‹æ± 

3\. ä¿å­˜åˆ°æ–‡ä»¶

è¿è¡Œæ•ˆæœ

å®Œæ•´ä»£ç 

ä¸¾ä¸€åä¸‰

å¤šè¿›ç¨‹+xpath

å¤šçº¿ç¨‹+bs4

å¤šè¿›ç¨‹+bs4

æ€»ç»“

* * *

### å‰è¨€

æˆ‘ä»¬å­¦ä¹ äº†å¤šçº¿ç¨‹ã€å¤šè¿›ç¨‹å¯¹äºæé«˜æ•ˆç‡çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ç°åœ¨å°±æ¥å°è¯•æ‰¹é‡æŠ“å–ä¹‹å‰å°è¯•è¿‡çš„æŸèœä»·ä¿¡æ¯ç½‘ç«™çš„å¤§é‡èœä»·ä¿¡æ¯**ï¼ˆé“¾æ¥æ”¾åœ¨è¯„è®ºåŒºï¼‰**

* * *

### ç›®çš„

ç”¨å¤šçº¿ç¨‹æ‰¹é‡æŠ“å–èœä»·ä¿¡æ¯

* * *

### æ€è·¯

1\. å®ç°æŠ“å–å•ä¸ªé¡µé¢çš„å‡½æ•°

2\. åˆ›å»ºçº¿ç¨‹æ± ï¼Œæ‰¹é‡æ‰§è¡Œä¸Šè¿°å‡½æ•°

3\. å†™å…¥æ–‡ä»¶

**æ³¨ï¼šæˆ‘å°†è¯¦ç»†è®²è¿°å¤šçº¿ç¨‹æ³•ï¼Œä¹‹åå±•ç¤ºå¤šè¿›ç¨‹æ³•ï¼Œå¹¶ä¸”æ•°æ®è§£ææ–¹å¼ä¼šç”¨bs4å’Œxpathä¸¤ç§æ–¹æ³•ã€‚**

* * *

### ä»£ç å®ç°ï¼ˆå¤šçº¿ç¨‹+xpathï¼‰

#### 1\. æŠ“å–å•ä¸ªé¡µé¢

![](/images/posts/21.-å®æˆ˜ï¼šå¤šçº¿ç¨‹+xpathæŠ“å–å¤§é‡èœä»·ä¿¡æ¯ï¼ˆå››ç§æ–¹æ³•ï¼‰/d0d623aba268.png)

è¿˜æ˜¯æ£€æŸ¥å…ƒç´ å¤åˆ¶xpathï¼Œå…·ä½“å‚è€ƒ[xpathå®ä¾‹å‚è€ƒæ–‡æ¡£](https://blog.csdn.net/m0_59180666/article/details/128527232?spm=1001.2014.3001.5501 "xpathå®ä¾‹å‚è€ƒæ–‡æ¡£") ï¼Œè¿™é‡Œå°±ä¸èµ˜è¿°äº†

```python def download_one_page(url): # æ‹¿åˆ°é¡µé¢æºä»£ç  resp = requests.get(url, headers=ua) html = etree.HTML(resp.text) table_body = html.xpath("/html/body/div/div[4]/div/div[2]/div[2]/table/tbody")[0] # trs = table_body.xpath("./tr") trs = table_body.xpath("./tr") # æ‹¿åˆ°æ¯ä¸ªtr for tr in trs: txt = tr.xpath("./td/text()") # print(txt) # æŠŠæ•°æ®å­˜æ”¾åœ¨æ–‡ä»¶ä¸­ csvwriter.writerow(txt) print(url, "æå–å®Œæ¯•!") ``` 

#### 2\. åˆ›å»ºçº¿ç¨‹æ± 

çº¿ç¨‹æ± ç­‰ç›¸å…³çŸ¥è¯†å‚ç…§[çº¿ç¨‹æ± ä¸è¿›ç¨‹æ± ](https://blog.csdn.net/m0_59180666/article/details/128581928?spm=1001.2014.3001.5501 "çº¿ç¨‹æ± ä¸è¿›ç¨‹æ± ")

```python if __name__ == '__main__': # for i in range(1, 1145): # æ•ˆç‡åŠå…¶ä½ä¸‹ # download_one_page(f"http://â€œè§è¯„è®ºåŒºâ€/import/list-1_{i}.html") # åˆ›å»ºçº¿ç¨‹æ±  with ThreadPoolExecutor(50) as t: for i in range(1, 200): # æŠŠä¸‹è½½ä»»åŠ¡æäº¤ç»™çº¿ç¨‹æ±  t.submit(download_one_page, f"http://â€œè§è¯„è®ºåŒºâ€/import/list-1_{i}.html") f.close() print("å…¨éƒ¨ä¸‹è½½å®Œæ¯•!") ``` 

è¿™é‡Œæˆ‘å°±åªçˆ¬200é¡µï¼Œæƒ³çˆ¬æ›´å¤šçš„è¯å°±æ”¹æ•°æ®ã€‚

#### 3\. ä¿å­˜åˆ°æ–‡ä»¶

```python f = open("4_price_data.csv", mode="w", encoding="utf-8", newline="") csvwriter = csv.writer(f) ``` 

* * *

### è¿è¡Œæ•ˆæœ

![](/images/posts/21.-å®æˆ˜ï¼šå¤šçº¿ç¨‹+xpathæŠ“å–å¤§é‡èœä»·ä¿¡æ¯ï¼ˆå››ç§æ–¹æ³•ï¼‰/1f3e425e85e3.png)

å¯ä»¥çœ‹åˆ°æå–é€Ÿåº¦è¿˜æ˜¯ç‰¹åˆ«å¿«çš„ï¼

* * *

### å®Œæ•´ä»£ç 

```python # 1. å¦‚ä½•æå–å•ä¸ªé¡µé¢çš„æ•°æ® # 2. ä¸Šçº¿ç¨‹æ± ,å¤šä¸ªé¡µé¢åŒæ—¶æŠ“å– import requests from lxml import etree import csv from concurrent.futures import ThreadPoolExecutor f = open("4_price_data.csv", mode="w", encoding="utf-8", newline="") csvwriter = csv.writer(f) ua = { "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.81 Safari/537.36 Edg/104.0.1293.54" } def download_one_page(url): # æ‹¿åˆ°é¡µé¢æºä»£ç  resp = requests.get(url, headers=ua) html = etree.HTML(resp.text) table_body = html.xpath("/html/body/div/div[4]/div/div[2]/div[2]/table/tbody")[0] # trs = table_body.xpath("./tr") trs = table_body.xpath("./tr") # æ‹¿åˆ°æ¯ä¸ªtr for tr in trs: txt = tr.xpath("./td/text()") # print(txt) # æŠŠæ•°æ®å­˜æ”¾åœ¨æ–‡ä»¶ä¸­ csvwriter.writerow(txt) print(url, "æå–å®Œæ¯•!") if __name__ == '__main__': # for i in range(1, 1145): # æ•ˆç‡åŠå…¶ä½ä¸‹ # download_one_page(f"http://â€œè§è¯„è®ºåŒºâ€/import/list-1_1.html") # åˆ›å»ºçº¿ç¨‹æ±  with ThreadPoolExecutor(50) as t: for i in range(1, 200): # æŠŠä¸‹è½½ä»»åŠ¡æäº¤ç»™çº¿ç¨‹æ±  t.submit(download_one_page, f"http://â€œè§è¯„è®ºåŒºâ€/import/list-1_{i}.html") f.close() print("å…¨éƒ¨ä¸‹è½½å®Œæ¯•!") ``` 

* * *

### ä¸¾ä¸€åä¸‰

#### å¤šè¿›ç¨‹+xpath

```python # 1. å¦‚ä½•æå–å•ä¸ªé¡µé¢çš„æ•°æ® # 2. ä¸Šçº¿ç¨‹æ± ,å¤šä¸ªé¡µé¢åŒæ—¶æŠ“å– import requests from lxml import etree import csv from concurrent.futures import ProcessPoolExecutor f = open("4_price_data.csv", mode="w", encoding="utf-8", newline="") csvwriter = csv.writer(f) ua = { "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.81 Safari/537.36 Edg/104.0.1293.54" } def download_one_page(url): # æ‹¿åˆ°é¡µé¢æºä»£ç  resp = requests.get(url, headers=ua) html = etree.HTML(resp.text) table_body = html.xpath("/html/body/div/div[4]/div/div[2]/div[2]/table/tbody")[0] # trs = table_body.xpath("./tr") trs = table_body.xpath("./tr") # æ‹¿åˆ°æ¯ä¸ªtr for tr in trs: txt = tr.xpath("./td/text()") # print(txt) # æŠŠæ•°æ®å­˜æ”¾åœ¨æ–‡ä»¶ä¸­ csvwriter.writerow(txt) print(url, "æå–å®Œæ¯•!") if __name__ == '__main__': # for i in range(1, 1145): # æ•ˆç‡åŠå…¶ä½ä¸‹ # download_one_page(f"http://â€œè§è¯„è®ºåŒºâ€/import/list-1_1.html") # åˆ›å»ºçº¿ç¨‹æ±  with ProcessPoolExecutor(50) as t: for i in range(1, 200): # æŠŠä¸‹è½½ä»»åŠ¡æäº¤ç»™çº¿ç¨‹æ±  t.submit(download_one_page, f"http://â€œè§è¯„è®ºåŒºâ€/import/list-1_{i}.html") f.close() print("å…¨éƒ¨ä¸‹è½½å®Œæ¯•!") ``` 

#### å¤šçº¿ç¨‹+bs4

```python # 1. å¦‚ä½•æå–å•ä¸ªé¡µé¢çš„æ•°æ® # 2. ä¸Šçº¿ç¨‹æ± ,å¤šä¸ªé¡µé¢åŒæ—¶æŠ“å– import requests from bs4 import BeautifulSoup import csv from concurrent.futures import ThreadPoolExecutor f = open("4_price_data.csv", mode="w", encoding="utf-8", newline="") csvwriter = csv.writer(f) ua = { "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.81 Safari/537.36 Edg/104.0.1293.54" } def download_one_page(url): # ä½¿ç”¨bs4è§£ææ•°æ®ï¼ˆä¸¤æ­¥ï¼‰ # 1. ç”Ÿæˆbså¯¹è±¡ resp = requests.get(url) page = BeautifulSoup(resp.text, "html.parser") # æŒ‡å®šhtmlè§£æå™¨ # 2. ä»bså¯¹è±¡ä¸­æŸ¥æ‰¾æ•°æ® # find(æ ‡ç­¾, å±æ€§=å€¼)ï¼šæ‰¾ç¬¬ä¸€ä¸ª # find_all(æ ‡ç­¾, å±æ€§=å€¼)ï¼šæ‰¾å…¨éƒ¨ # table = page.find("table",class_="price-table") # classæ˜¯pythonä¸­çš„å…³é”®å­—ï¼ŒåŠ _ä»¥ç¤ºåŒºåˆ« # å¦ä¸€ç§å†™æ³•ï¼š table = page.find("table", attrs={"class": "price-table"}) # å’Œä¸Šä¸€è¡Œæ˜¯ä¸€ä¸ªæ„æ€ï¼Œæ­¤æ—¶å¯ä»¥é¿å…class # print(table) # ä¸æƒ³è¦åˆ—åé‚£ä¸€è¡Œï¼ˆè¡¨å¤´ï¼‰ï¼Œåªæƒ³è¦åº•ä¸‹çš„æ•°æ®ï¼Œå³æ‹¿åˆ°æ‰€æœ‰æ•°æ®è¡Œ trs = table.find_all("tr")[1:] # træ˜¯è¡Œçš„æ„æ€ for tr in trs: # æ¯ä¸€è¡Œ tds = tr.find_all("td") # tdè¡¨ç¤ºå•å…ƒæ ¼ã€‚æ‹¿åˆ°æ¯è¡Œä¸­çš„æ‰€æœ‰td # print(tds[0]) # åå­—ã€äº§åœ°ã€å‡ä»·ï¼ˆå…ƒ/å…¬æ–¤ï¼‰ã€è§„æ ¼ã€æ—¥æœŸ name = tds[0].text # .textè¡¨ç¤ºæ‹¿åˆ°è¢«æ ‡ç­¾æ ‡è®°çš„å†…å®¹ place = tds[1].text avg_price = tds[2].text spec = tds[3].text date = tds[4].text # print(name,place,avg_price,spec,date) csvwriter.writerow([name, place, avg_price, spec, date]) print(url, "æå–å®Œæ¯•!") if __name__ == '__main__': # for i in range(1, 1145): # æ•ˆç‡åŠå…¶ä½ä¸‹ # download_one_page(f"http://â€œè§è¯„è®ºåŒºâ€/import/list-1_1.html") # åˆ›å»ºçº¿ç¨‹æ±  with ThreadPoolExecutor(50) as t: for i in range(1, 200): # æŠŠä¸‹è½½ä»»åŠ¡æäº¤ç»™çº¿ç¨‹æ±  t.submit(download_one_page, f"http://â€œè§è¯„è®ºåŒºâ€/import/list-1_{i}.html") f.close() print("å…¨éƒ¨ä¸‹è½½å®Œæ¯•!") ``` 

#### å¤šè¿›ç¨‹+bs4

```python # 1. å¦‚ä½•æå–å•ä¸ªé¡µé¢çš„æ•°æ® # 2. ä¸Šçº¿ç¨‹æ± ,å¤šä¸ªé¡µé¢åŒæ—¶æŠ“å– import requests from bs4 import BeautifulSoup import csv from concurrent.futures import ProcessPoolExecutor f = open("4_price_data.csv", mode="w", encoding="utf-8", newline="") csvwriter = csv.writer(f) ua = { "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.81 Safari/537.36 Edg/104.0.1293.54" } def download_one_page(url): # ä½¿ç”¨bs4è§£ææ•°æ®ï¼ˆä¸¤æ­¥ï¼‰ # 1. ç”Ÿæˆbså¯¹è±¡ resp = requests.get(url) page = BeautifulSoup(resp.text, "html.parser") # æŒ‡å®šhtmlè§£æå™¨ # 2. ä»bså¯¹è±¡ä¸­æŸ¥æ‰¾æ•°æ® # find(æ ‡ç­¾, å±æ€§=å€¼)ï¼šæ‰¾ç¬¬ä¸€ä¸ª # find_all(æ ‡ç­¾, å±æ€§=å€¼)ï¼šæ‰¾å…¨éƒ¨ # table = page.find("table",class_="price-table") # classæ˜¯pythonä¸­çš„å…³é”®å­—ï¼ŒåŠ _ä»¥ç¤ºåŒºåˆ« # å¦ä¸€ç§å†™æ³•ï¼š table = page.find("table", attrs={"class": "price-table"}) # å’Œä¸Šä¸€è¡Œæ˜¯ä¸€ä¸ªæ„æ€ï¼Œæ­¤æ—¶å¯ä»¥é¿å…class # print(table) # ä¸æƒ³è¦åˆ—åé‚£ä¸€è¡Œï¼ˆè¡¨å¤´ï¼‰ï¼Œåªæƒ³è¦åº•ä¸‹çš„æ•°æ®ï¼Œå³æ‹¿åˆ°æ‰€æœ‰æ•°æ®è¡Œ trs = table.find_all("tr")[1:] # træ˜¯è¡Œçš„æ„æ€ for tr in trs: # æ¯ä¸€è¡Œ tds = tr.find_all("td") # tdè¡¨ç¤ºå•å…ƒæ ¼ã€‚æ‹¿åˆ°æ¯è¡Œä¸­çš„æ‰€æœ‰td # print(tds[0]) # åå­—ã€äº§åœ°ã€å‡ä»·ï¼ˆå…ƒ/å…¬æ–¤ï¼‰ã€è§„æ ¼ã€æ—¥æœŸ name = tds[0].text # .textè¡¨ç¤ºæ‹¿åˆ°è¢«æ ‡ç­¾æ ‡è®°çš„å†…å®¹ place = tds[1].text avg_price = tds[2].text spec = tds[3].text date = tds[4].text # print(name,place,avg_price,spec,date) csvwriter.writerow([name, place, avg_price, spec, date]) print(url, "æå–å®Œæ¯•!") if __name__ == '__main__': # for i in range(1, 1145): # æ•ˆç‡åŠå…¶ä½ä¸‹ # download_one_page(f"http://â€œè§è¯„è®ºåŒºâ€/import/list-1_1.html") # åˆ›å»ºçº¿ç¨‹æ±  with ProcessPoolExecutor(50) as t: for i in range(1, 200): # æŠŠä¸‹è½½ä»»åŠ¡æäº¤ç»™çº¿ç¨‹æ±  t.submit(download_one_page, f"http://â€œè§è¯„è®ºåŒºâ€/import/list-1_{i}.html") f.close() print("å…¨éƒ¨ä¸‹è½½å®Œæ¯•!") ``` 

* * *

### **æ€»ç»“**

æˆ‘ä»¬ä»Šå¤©é€šè¿‡å®æˆ˜æ‰¹é‡è·å–äº†æŸç½‘ç«™å¤§é‡çš„èœä»·ä¿¡æ¯ï¼Œå®è·µäº†bs4ã€xpathã€çº¿ç¨‹æ± ã€è¿›ç¨‹æ± çš„åº”ç”¨ã€‚**è®¿é—®çš„ç½‘ç«™è§è¯„è®ºåŒºï¼ï¼ï¼**
