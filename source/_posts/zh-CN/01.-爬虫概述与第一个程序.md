---
title: "01. 爬虫概述与第一个程序"
date: 2022-12-27
updated: 2022-12-27
categories:
  - Python爬虫入门、进阶与实战
tags:
  - 爬虫
  - python
  - 开发语言
  - html5
csdn_views: 510
csdn_likes: 4
csdn_comments: 1
csdn_favorites: 3
csdn_url: https://blog.csdn.net/m0_59180666/article/details/128459026
lang_pair:
  en: "01. Web Crawler Overview and First Program"
---

> 本文迁移自CSDN博客
> 原文链接：[01. 爬虫概述与第一个程序](https://blog.csdn.net/m0_59180666/article/details/128459026)
> 📊 510 阅读 | 👍 4 点赞 | 💬 1 评论 | ⭐ 3 收藏

**目录**

爬虫概述

什么是爬虫?

爬虫和Python

爬虫合法么?

爬虫的矛与盾

反爬机制

反反爬策略

我的第一个爬虫相关程序

* * *

## 爬虫概述

* * *

### **什么是爬虫?**

不知道各位是否遇到过这样的需求. 就是我们总是希望能够保存互联网上的⼀些重要的数据信息为己所用。

比如，

>   * 在浏览到⼀些优秀好看的图片时，总想保存起来留为日后做桌面上的壁纸；
>   * 在浏览到⼀些重要的数据时(各行各业)，希望保留下来日后为自己进行各种销售行为增光添彩；
>   * 在浏览到⼀些感兴趣的视频时, 希望保存在硬盘里供日后慢慢品鉴 ；
>   * 在浏览到⼀些十分优秀的歌声曲目时, 希望保存下来供我们在烦闷的生活中增添⼀份精彩
>   * ......
> 

那么恭喜你！爬虫将特别适合于你. 因为爬虫就是通过编写程序来爬取互联⽹上的优秀资源(图片、视频、音频、数据...)

* * *

### **爬虫和Python**

爬虫⼀定要用Python么? 非也~ 用Java也行, C也可以. 请各位记住, 编程语言只是工具. 抓到数据是你的目的。用什么工具去达到你的目 的都是可以的。和吃饭⼀样, 可以用叉子也可以用筷子, 最终的结果都是你能吃到饭。

那为什么大多数人喜欢用Python呢?

答案: 因为 Python写爬虫简单。不理解? 问: 为什么吃米饭不用刀叉? 用筷子? 因为简单! 好用! 而Python是众多编程语言中, 小白上手最快, 语法最简单，更重要的 是, 这货有非常多的关于爬虫能用到的第三方支持库. 说直白点儿就是你用筷子吃饭, 我还附送你⼀个佣人帮你吃! 这样吃的是不是更爽了、更容易了~

* * *

### **爬虫合法么?**

首先，爬虫在法律上是不被禁止的，也就是说法律是允许爬虫存在的。但是，爬虫也具有违法风险的。就像菜刀⼀样，法律是允许菜刀的存在的，但是你要是用来犯事，那对不起，没人惯着你。就像王欣说过的：

> 技术是无罪的。主要看你用它来干嘛。

比如有些人就利用爬虫+⼀些黑客技术每秒对着网站撸上十万八千次，那这个肯定是不被允许的。

爬虫分为善意的爬虫和恶意的爬虫 

  * 善意的爬虫, 不破坏被爬取的网站的资源(正常访问, ⼀般频率不高, 不窃取用户隐私) 
  * 恶意的爬虫, 影响网站的正常运营(抢票, 秒杀, 疯狂solo网站资源，造成网站宕机)

综上, 为了避免进橘子我们还是要安分守己，时常优化自己的爬虫程序，避免干扰到网站的正常运行。并且在使用爬取到的数据时，发现涉及到用户隐私和商业机密等敏感内容时，⼀定要及时终止爬取和传播！

* * *

### **爬虫的矛与盾**

#### 反爬机制

门户网站可以通过制定相应的策略或者技术手段，防止爬虫程序进行网站数据的爬取。 

#### **反反爬策略**

爬虫程序可以通过制定相关的策略或者技术手段，破解门户网站中具备的反爬机制，从而可以获取门户网站中相关的数据。 

robots.txt协议： 君子协议。规定了网站中哪些数据可以被爬虫爬取，哪些数据不可以被爬取。

* * *

### 我的第一个爬虫相关程序

```python # Created at UESTC # Author: Vector Kun # Time: 2022/10/29 20:45 # 我的第一个python爬虫程序： # 需求：用程序模拟浏览器，输入一个网址，从中获取到资源或者内容 from urllib.request import urlopen url = "http://www.baidu.com" resp = urlopen(url) # print(resp.read()) //这样写得不到正常数据，是汉字的中文代码，需要转码，由于“content="text/html;charset=utf-8”所以用utf-8转码 # print(resp.read().decode("utf-8")) //这样输出到控制台依旧难以辨识，选择输出成html文件便于阅读 with open("1_mybaidu.html", mode="w", encoding="utf-8") as f: f.write(resp.read().decode("utf-8")) #读取到网页的页面源代码 print("over!") # 运行html文件就可以运行这个网页了 resp.close() ``` 

运行完成后可以自动生成url为百度搜索的html文件，并保存在本py文件相同路径下，运行这个html文件就可以看到百度搜索首页了！

第20行的resp.close()是关闭当前网络请求，避免后续网络端口被占用太多而无法进行网络请求。

以后我们写爬虫程序的时候，最好养成写resp.close()的习惯，这和C语言中malloc后再free内存是同样重要的！
